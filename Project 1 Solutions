Solutions to ACS Project 1
9/22/2023
Andrew Prata, Naif Jabir

(1) Read and Write Latency for Cache and DRAM
_____________________________________________

Zero queueing delay implies that there is no wait for
data writing or access. This means that the system
should be idling, or as close to idling as possible
to get an accurate reading of unloaded latencies.

The size of the caches for the target processor,
an Intel Core i7-12700K, are as follows:
L1: 1.0 MB
L2: 12.0 MB
L3: 25.0 MB

To test latencies, we will use the MLC commands below:
Testing L1
mlc --idle_latency -b900k

Testing L2
mlc --idle_latency -b11m

Testing L3
mlc --idle_latency -b23m

Testing DRAM
mlc --idle_latency -b60m

These commands yielded the following data, averaged
over three runs each:
L1 Cache Read Latency: 3.93 ns
L2 Cache Read Latency: 16.87 ns
L3 Cache Read Latency: 36.30 ns
DRAM Read Latency:     69.87 ns

(2) DRAM Maximum Bandwidth
__________________________

The Intel MLC uses the size of the allocated buffer
to determine the Bandwidth of a particular memory
hierarchy level. Need to find out the command line
parameters to configure the size of this. Then, can
determine what the latencies of different levels are.
PAGE 9 OF MLC DOCUMENT

we measure all write for default value of 64 bytes
--bandwidth_matrix -w6

max bandwidth we can get with stride byte size of 64 (for all read, 3:1 read, 2:1 read, 1:1 read)
--peak_injection_bandwidth -X -b100m -l64

max bandwidth we can get with stride byte size of 256 (for all read, 3:1 read, 2:1 read, 1:1 read)
--peak_injection_bandwidth -X -b100m -l256

max bandwidth we can get with stride byte size of 1024 (for all read, 3:1 read, 2:1 read, 1:1 read)
--peak_injection_bandwidth -X -b100m -l1024

according to the readme, the bandwidth matrix command is supposed to be able to use -ln and -Wn 
together, but it results in an error so we have no way to analyze all write bandwidth for
256 bytes and 1024 bytes
The other commands (max_bandwidth and peak_injection_bandwidth) cannot use -Wn parameter

(3) The Tradeoff Between Latency and Bandwidth of DRAM
______________________________________________________

(initally we tried to code the queuing theory, we had a number of tasks and gave we tasks at
certain amount of delay to prevent pile up at the queue, but no matter how many tasks we gave
whether it was 12 tasks or 250 tasks, the throughput and latency seemed to stay around the same
value and it was giving us the opposite of what the queuing theory predicts)
XXXXXX

(we decided coding the queuing theory was too difficult and looked through the mlc DOCUMENT
again to see what commands could help us get both throughput and latency)

--loaded_latency
we will get a certain bandwidth accosiated with a certain latency and show how this data
mimmicks the queuing theory (at first, from 0-25,000 mb/s, we get relatively the same latency
of 100-150, but once we should the bandwidth to 35,000 and higher, the latency increases at
an exponential rate)
(queuing theory predicts as the system throughput increases linearly, we will eventually see a
point in the system, where the response time aka latency increases exponentially instead of 
staying around the same value)

(4) Impact of Cache Miss Ratio on Performance


(5) Impact of TLB Miss Ratio on Performance

